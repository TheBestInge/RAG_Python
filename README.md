# Reqs

- vllm: For serving locally the LLM
- transformers:
- huggingface_hub: To acces HF hub

# How is it build

## LLM backend

For this we use vLLM

https://docs.vllm.ai/en/latest/